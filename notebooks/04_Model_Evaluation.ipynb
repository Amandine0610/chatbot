{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Healthcare Chatbot - Model Evaluation\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Load your trained healthcare chatbot\n",
    "2. Run comprehensive evaluation metrics\n",
    "3. Test conversational quality\n",
    "4. Analyze performance and get improvement suggestions\n",
    "5. Compare with baseline models\n",
    "\n",
    "Let's see how well your AI doctor performs! ü©∫üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Setup and Model Loading\n",
    "\n",
    "First, let's set up the environment and load your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('/workspace/src')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üîß Evaluation environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available trained models\n",
    "def find_trained_models():\n",
    "    \"\"\"Find trained model directories.\"\"\"\n",
    "    models_dir = '/workspace/models'\n",
    "    trained_models = []\n",
    "    \n",
    "    if os.path.exists(models_dir):\n",
    "        for item in os.listdir(models_dir):\n",
    "            model_path = os.path.join(models_dir, item)\n",
    "            if os.path.isdir(model_path):\n",
    "                # Look for final_model subdirectory\n",
    "                final_model_path = os.path.join(model_path, 'final_model')\n",
    "                if os.path.exists(final_model_path):\n",
    "                    trained_models.append(final_model_path)\n",
    "                # Or check if it's directly a model directory\n",
    "                elif any(f.endswith('.bin') or f.endswith('.safetensors') for f in os.listdir(model_path)):\n",
    "                    trained_models.append(model_path)\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "print(\"üîç Searching for trained models...\")\n",
    "available_models = find_trained_models()\n",
    "\n",
    "if available_models:\n",
    "    print(f\"üìÅ Found {len(available_models)} trained model(s):\")\n",
    "    for i, model_path in enumerate(available_models, 1):\n",
    "        # Get model info\n",
    "        model_name = os.path.basename(os.path.dirname(model_path)) if model_path.endswith('final_model') else os.path.basename(model_path)\n",
    "        \n",
    "        # Check for training summary\n",
    "        summary_path = os.path.join(os.path.dirname(model_path), 'kaggle_training_summary.json')\n",
    "        if not os.path.exists(summary_path):\n",
    "            summary_path = os.path.join(os.path.dirname(model_path), 'training_summary.json')\n",
    "        \n",
    "        timestamp = \"Unknown\"\n",
    "        if os.path.exists(summary_path):\n",
    "            try:\n",
    "                with open(summary_path, 'r') as f:\n",
    "                    summary = json.load(f)\n",
    "                timestamp = summary.get('timestamp', 'Unknown')\n",
    "                if timestamp != 'Unknown':\n",
    "                    timestamp = datetime.fromisoformat(timestamp).strftime('%Y-%m-%d %H:%M')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"   {i}. {model_name} (trained: {timestamp})\")\n",
    "        print(f\"      Path: {model_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No trained models found.\")\n",
    "    print(\"\\nüîß SOLUTIONS:\")\n",
    "    print(\"1. Train a model first using notebook 03_Model_Training.ipynb\")\n",
    "    print(\"2. Or run: python train_kaggle_chatbot.py --kaggle_dataset_path your-dataset.csv\")\n",
    "    print(\"3. Check that training completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model for evaluation\n",
    "# MODIFY THIS: Set the path to your trained model\n",
    "MODEL_PATH = None\n",
    "\n",
    "# Auto-select the most recent model if available\n",
    "if available_models:\n",
    "    MODEL_PATH = available_models[0]  # Use the first (most recent) model\n",
    "    print(f\"ü§ñ Auto-selected model: {MODEL_PATH}\")\n",
    "\n",
    "# Or manually set the path\n",
    "# MODEL_PATH = '/workspace/models/my_healthcare_chatbot/final_model'\n",
    "\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(f\"‚úÖ Model selected for evaluation: {os.path.basename(os.path.dirname(MODEL_PATH))}\")\n",
    "    \n",
    "    # Load model info if available\n",
    "    model_info = {}\n",
    "    summary_path = os.path.join(os.path.dirname(MODEL_PATH), 'kaggle_training_summary.json')\n",
    "    if not os.path.exists(summary_path):\n",
    "        summary_path = os.path.join(os.path.dirname(MODEL_PATH), 'training_summary.json')\n",
    "    \n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            model_info = json.load(f)\n",
    "        print(\"üìã Training information loaded\")\n",
    "    \n",
    "    model_selected = True\n",
    "else:\n",
    "    print(\"‚ùå Please set MODEL_PATH to your trained model directory.\")\n",
    "    print(\"   Example: MODEL_PATH = '/workspace/models/my_healthcare_chatbot/final_model'\")\n",
    "    model_selected = False\n",
    "    model_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "if model_selected:\n",
    "    print(\"üîÑ LOADING TRAINED MODEL\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    try:\n",
    "        from evaluation import HealthcareEvaluator\n",
    "        from chatbot import HealthcareChatbot\n",
    "        \n",
    "        # Initialize evaluator\n",
    "        print(\"üîÑ Initializing evaluator...\")\n",
    "        evaluator = HealthcareEvaluator(MODEL_PATH)\n",
    "        \n",
    "        # Initialize chatbot for interactive testing\n",
    "        print(\"üîÑ Initializing chatbot...\")\n",
    "        chatbot = HealthcareChatbot(MODEL_PATH)\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "        # Display model information\n",
    "        if model_info:\n",
    "            print(\"\\nüìä Model Information:\")\n",
    "            training_config = model_info.get('training_config', {})\n",
    "            if training_config:\n",
    "                print(f\"   Model type: {training_config.get('model_key', 'Unknown')}\")\n",
    "                print(f\"   Training samples: {model_info.get('dataset_stats', {}).get('training_samples', 'Unknown')}\")\n",
    "                print(f\"   Validation samples: {model_info.get('dataset_stats', {}).get('validation_samples', 'Unknown')}\")\n",
    "                \n",
    "                hyperparams = training_config.get('hyperparameters', {})\n",
    "                if hyperparams:\n",
    "                    print(f\"   Learning rate: {hyperparams.get('learning_rate', 'Unknown')}\")\n",
    "                    print(f\"   Epochs: {hyperparams.get('num_train_epochs', 'Unknown')}\")\n",
    "                    print(f\"   Batch size: {hyperparams.get('per_device_train_batch_size', 'Unknown')}\")\n",
    "        \n",
    "        model_loaded = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        print(\"\\nüîß TROUBLESHOOTING:\")\n",
    "        print(\"1. Check that the model path is correct\")\n",
    "        print(\"2. Ensure training completed successfully\")\n",
    "        print(\"3. Verify model files exist in the directory\")\n",
    "        \n",
    "        model_loaded = False\n",
    "        evaluator = None\n",
    "        chatbot = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot load model - no model selected\")\n",
    "    model_loaded = False\n",
    "    evaluator = None\n",
    "    chatbot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Quantitative Evaluation\n",
    "\n",
    "Let's run comprehensive metrics on your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "if model_loaded:\n",
    "    print(\"üìä PREPARING EVALUATION DATASET\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Try to find the dataset used for training\n",
    "    dataset_paths = [\n",
    "        '/workspace/data/kaggle_medical_dataset.json',\n",
    "        '/workspace/data/healthcare_qa_dataset.json'\n",
    "    ]\n",
    "    \n",
    "    eval_dataset = None\n",
    "    dataset_path = None\n",
    "    \n",
    "    for path in dataset_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    eval_dataset = json.load(f)\n",
    "                dataset_path = path\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if eval_dataset:\n",
    "        print(f\"‚úÖ Evaluation dataset loaded: {os.path.basename(dataset_path)}\")\n",
    "        print(f\"üìä Total samples: {len(eval_dataset)}\")\n",
    "        \n",
    "        # Limit samples for faster evaluation if dataset is large\n",
    "        max_eval_samples = 100\n",
    "        if len(eval_dataset) > max_eval_samples:\n",
    "            eval_dataset = eval_dataset[:max_eval_samples]\n",
    "            print(f\"üìä Limited to {max_eval_samples} samples for faster evaluation\")\n",
    "        \n",
    "        dataset_ready = True\n",
    "    else:\n",
    "        print(\"‚ùå No evaluation dataset found\")\n",
    "        print(\"\\nüîß SOLUTIONS:\")\n",
    "        print(\"1. Make sure you've processed your dataset (notebook 02)\")\n",
    "        print(\"2. Check dataset paths above\")\n",
    "        print(\"3. You can still do qualitative evaluation below\")\n",
    "        \n",
    "        dataset_ready = False\n",
    "        eval_dataset = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model not loaded - cannot prepare evaluation\")\n",
    "    dataset_ready = False\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quantitative evaluation\n",
    "if model_loaded and dataset_ready:\n",
    "    print(\"üß™ RUNNING QUANTITATIVE EVALUATION\")\n",
    "    print(\"=\"*45)\n",
    "    print(\"‚è±Ô∏è This may take a few minutes...\")\n",
    "    \n",
    "    try:\n",
    "        # Save dataset temporarily for evaluator\n",
    "        temp_dataset_path = '/tmp/eval_dataset.json'\n",
    "        with open(temp_dataset_path, 'w') as f:\n",
    "            json.dump(eval_dataset, f)\n",
    "        \n",
    "        # Run evaluation\n",
    "        eval_results = evaluator.evaluate_on_dataset(\n",
    "            temp_dataset_path,\n",
    "            num_samples=len(eval_dataset)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ EVALUATION COMPLETED!\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Display key metrics\n",
    "        metrics = {\n",
    "            'BLEU Score': eval_results.get('corpus_bleu', 0),\n",
    "            'ROUGE-1 F1': eval_results.get('rouge1_f1', 0),\n",
    "            'ROUGE-2 F1': eval_results.get('rouge2_f1', 0),\n",
    "            'ROUGE-L F1': eval_results.get('rougeL_f1', 0),\n",
    "            'Semantic Similarity': eval_results.get('semantic_similarity', 0),\n",
    "            'Perplexity': eval_results.get('perplexity', 0),\n",
    "            'Avg Prediction Length': eval_results.get('avg_prediction_length', 0),\n",
    "            'Avg Reference Length': eval_results.get('avg_reference_length', 0)\n",
    "        }\n",
    "        \n",
    "        print(\"üìä PERFORMANCE METRICS:\")\n",
    "        print(\"-\"*30)\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric:.<25} {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric:.<25} {value}\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        print(\"\\nüéØ PERFORMANCE ASSESSMENT:\")\n",
    "        print(\"-\"*35)\n",
    "        \n",
    "        bleu_score = eval_results.get('corpus_bleu', 0)\n",
    "        rouge_l = eval_results.get('rougeL_f1', 0)\n",
    "        similarity = eval_results.get('semantic_similarity', 0)\n",
    "        \n",
    "        assessments = []\n",
    "        \n",
    "        # BLEU assessment\n",
    "        if bleu_score > 0.3:\n",
    "            assessments.append(\"üü¢ BLEU: Excellent (>0.30)\")\n",
    "        elif bleu_score > 0.2:\n",
    "            assessments.append(\"üü° BLEU: Good (0.20-0.30)\")\n",
    "        elif bleu_score > 0.1:\n",
    "            assessments.append(\"üü† BLEU: Fair (0.10-0.20)\")\n",
    "        else:\n",
    "            assessments.append(\"üî¥ BLEU: Needs improvement (<0.10)\")\n",
    "        \n",
    "        # ROUGE assessment\n",
    "        if rouge_l > 0.4:\n",
    "            assessments.append(\"üü¢ ROUGE-L: Excellent (>0.40)\")\n",
    "        elif rouge_l > 0.3:\n",
    "            assessments.append(\"üü° ROUGE-L: Good (0.30-0.40)\")\n",
    "        elif rouge_l > 0.2:\n",
    "            assessments.append(\"üü† ROUGE-L: Fair (0.20-0.30)\")\n",
    "        else:\n",
    "            assessments.append(\"üî¥ ROUGE-L: Needs improvement (<0.20)\")\n",
    "        \n",
    "        # Similarity assessment\n",
    "        if similarity > 0.4:\n",
    "            assessments.append(\"üü¢ Similarity: Excellent (>0.40)\")\n",
    "        elif similarity > 0.3:\n",
    "            assessments.append(\"üü° Similarity: Good (0.30-0.40)\")\n",
    "        elif similarity > 0.2:\n",
    "            assessments.append(\"üü† Similarity: Fair (0.20-0.30)\")\n",
    "        else:\n",
    "            assessments.append(\"üî¥ Similarity: Needs improvement (<0.20)\")\n",
    "        \n",
    "        for assessment in assessments:\n",
    "            print(assessment)\n",
    "        \n",
    "        # Overall score\n",
    "        overall_score = (bleu_score + rouge_l + similarity) / 3\n",
    "        print(f\"\\nüìä Overall Score: {overall_score:.3f}\")\n",
    "        \n",
    "        if overall_score > 0.35:\n",
    "            print(\"üéâ Excellent performance! Your chatbot is working very well.\")\n",
    "        elif overall_score > 0.25:\n",
    "            print(\"üëç Good performance! Your chatbot is functioning well.\")\n",
    "        elif overall_score > 0.15:\n",
    "            print(\"üëå Fair performance. Consider improvements.\")\n",
    "        else:\n",
    "            print(\"üîß Performance needs improvement. See suggestions below.\")\n",
    "        \n",
    "        quantitative_completed = True\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(temp_dataset_path):\n",
    "            os.remove(temp_dataset_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {e}\")\n",
    "        print(\"\\nüîß TROUBLESHOOTING:\")\n",
    "        print(\"1. Check that the model is properly loaded\")\n",
    "        print(\"2. Reduce evaluation dataset size\")\n",
    "        print(\"3. Check available memory\")\n",
    "        \n",
    "        quantitative_completed = False\n",
    "        eval_results = {}\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot run quantitative evaluation - prerequisites not met\")\n",
    "    quantitative_completed = False\n",
    "    eval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "if quantitative_completed and eval_results:\n",
    "    print(\"üìä CREATING EVALUATION VISUALIZATIONS\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Create metrics visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Healthcare Chatbot Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Main metrics bar chart\n",
    "    main_metrics = {\n",
    "        'BLEU': eval_results.get('corpus_bleu', 0),\n",
    "        'ROUGE-L': eval_results.get('rougeL_f1', 0),\n",
    "        'Similarity': eval_results.get('semantic_similarity', 0)\n",
    "    }\n",
    "    \n",
    "    bars = axes[0, 0].bar(main_metrics.keys(), main_metrics.values(), \n",
    "                         color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    axes[0, 0].set_title('Main Performance Metrics')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, main_metrics.values()):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. ROUGE metrics breakdown\n",
    "    rouge_metrics = {\n",
    "        'ROUGE-1': eval_results.get('rouge1_f1', 0),\n",
    "        'ROUGE-2': eval_results.get('rouge2_f1', 0),\n",
    "        'ROUGE-L': eval_results.get('rougeL_f1', 0)\n",
    "    }\n",
    "    \n",
    "    bars = axes[0, 1].bar(rouge_metrics.keys(), rouge_metrics.values(), \n",
    "                         color=['orange', 'yellow', 'lightgreen'])\n",
    "    axes[0, 1].set_title('ROUGE Metrics Breakdown')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    for bar, value in zip(bars, rouge_metrics.values()):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Response length comparison\n",
    "    pred_length = eval_results.get('avg_prediction_length', 0)\n",
    "    ref_length = eval_results.get('avg_reference_length', 0)\n",
    "    \n",
    "    length_data = ['Predictions', 'References']\n",
    "    length_values = [pred_length, ref_length]\n",
    "    \n",
    "    bars = axes[1, 0].bar(length_data, length_values, color=['purple', 'brown'])\n",
    "    axes[1, 0].set_title('Average Response Length')\n",
    "    axes[1, 0].set_ylabel('Words')\n",
    "    \n",
    "    for bar, value in zip(bars, length_values):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                       f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Performance radar chart (simplified as bar chart)\n",
    "    performance_categories = {\n",
    "        'Accuracy': min(eval_results.get('corpus_bleu', 0) * 2, 1),  # Scale BLEU\n",
    "        'Fluency': min(eval_results.get('rougeL_f1', 0) * 1.5, 1),   # Scale ROUGE-L\n",
    "        'Relevance': eval_results.get('semantic_similarity', 0),\n",
    "        'Length': min(abs(1 - eval_results.get('length_ratio', 1)), 1)  # Length appropriateness\n",
    "    }\n",
    "    \n",
    "    bars = axes[1, 1].bar(performance_categories.keys(), performance_categories.values(),\n",
    "                         color=['red', 'blue', 'green', 'orange'])\n",
    "    axes[1, 1].set_title('Performance Categories')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, performance_categories.values()):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations created!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No evaluation results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó£Ô∏è Step 3: Qualitative Evaluation\n",
    "\n",
    "Let's test your chatbot with real conversations and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test questions\n",
    "if model_loaded:\n",
    "    print(\"üó£Ô∏è QUALITATIVE EVALUATION\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Comprehensive test questions covering different medical areas\n",
    "    test_questions = [\n",
    "        # Basic symptoms\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"How do I know if I have high blood pressure?\",\n",
    "        \"What causes chest pain?\",\n",
    "        \n",
    "        # Treatment and medication\n",
    "        \"How is pneumonia treated?\",\n",
    "        \"What are the side effects of aspirin?\",\n",
    "        \"How long should I take antibiotics?\",\n",
    "        \n",
    "        # Prevention and wellness\n",
    "        \"How can I prevent heart disease?\",\n",
    "        \"What foods boost immunity?\",\n",
    "        \"How much exercise do I need daily?\",\n",
    "        \n",
    "        # Emergency situations\n",
    "        \"What should I do if someone has a heart attack?\",\n",
    "        \"When should I go to the emergency room?\",\n",
    "        \n",
    "        # Out-of-domain tests\n",
    "        \"What's the weather like today?\",\n",
    "        \"Can you help me with my math homework?\",\n",
    "        \"What's the capital of France?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üß™ Testing with {len(test_questions)} questions...\")\n",
    "    print(\"\\nü§ñ CHATBOT RESPONSES:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    qualitative_results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i:2d}. üë§ User: {question}\")\n",
    "        \n",
    "        try:\n",
    "            response = chatbot.chat(question)\n",
    "            answer = response['response']\n",
    "            response_type = response['type']\n",
    "            \n",
    "            # Determine if this is a healthcare question\n",
    "            is_medical = response_type == 'healthcare'\n",
    "            \n",
    "            # Color coding for response types\n",
    "            if response_type == 'healthcare':\n",
    "                icon = \"üè•\"\n",
    "            elif response_type == 'out_of_domain':\n",
    "                icon = \"‚ö†Ô∏è\"\n",
    "            else:\n",
    "                icon = \"üí¨\"\n",
    "            \n",
    "            print(f\"    {icon} Bot: {answer}\")\n",
    "            print(f\"    üìä Type: {response_type}\")\n",
    "            \n",
    "            # Store result for analysis\n",
    "            qualitative_results.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'type': response_type,\n",
    "                'is_medical': is_medical,\n",
    "                'is_out_of_domain': 'weather' in question.lower() or 'math' in question.lower() or 'capital' in question.lower()\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error: {e}\")\n",
    "            qualitative_results.append({\n",
    "                'question': question,\n",
    "                'answer': f\"Error: {e}\",\n",
    "                'type': 'error',\n",
    "                'is_medical': False,\n",
    "                'is_out_of_domain': False\n",
    "            })\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    qualitative_completed = True\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot run qualitative evaluation - model not loaded\")\n",
    "    qualitative_completed = False\n",
    "    qualitative_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze qualitative results\n",
    "if qualitative_completed and qualitative_results:\n",
    "    print(\"üìä QUALITATIVE ANALYSIS\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Calculate response type accuracy\n",
    "    medical_questions = [r for r in qualitative_results if not r['is_out_of_domain'] and r['type'] != 'error']\n",
    "    out_of_domain_questions = [r for r in qualitative_results if r['is_out_of_domain']]\n",
    "    \n",
    "    # Medical question handling\n",
    "    medical_correct = sum(1 for r in medical_questions if r['is_medical'])\n",
    "    medical_total = len(medical_questions)\n",
    "    medical_accuracy = medical_correct / medical_total if medical_total > 0 else 0\n",
    "    \n",
    "    # Out-of-domain handling\n",
    "    ood_correct = sum(1 for r in out_of_domain_questions if not r['is_medical'])\n",
    "    ood_total = len(out_of_domain_questions)\n",
    "    ood_accuracy = ood_correct / ood_total if ood_total > 0 else 0\n",
    "    \n",
    "    # Response length analysis\n",
    "    response_lengths = [len(r['answer'].split()) for r in qualitative_results if r['type'] != 'error']\n",
    "    avg_length = np.mean(response_lengths) if response_lengths else 0\n",
    "    \n",
    "    print(\"üìà RESPONSE ANALYSIS:\")\n",
    "    print(\"-\"*25)\n",
    "    print(f\"Medical Question Accuracy: {medical_accuracy:.1%} ({medical_correct}/{medical_total})\")\n",
    "    print(f\"Out-of-Domain Handling: {ood_accuracy:.1%} ({ood_correct}/{ood_total})\")\n",
    "    print(f\"Average Response Length: {avg_length:.1f} words\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    overall_accuracy = (medical_accuracy + ood_accuracy) / 2\n",
    "    print(f\"\\nüéØ Overall Conversational Accuracy: {overall_accuracy:.1%}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(\"\\nüîç QUALITY ASSESSMENT:\")\n",
    "    print(\"-\"*25)\n",
    "    \n",
    "    if medical_accuracy >= 0.8:\n",
    "        print(\"üü¢ Medical Response Quality: Excellent\")\n",
    "    elif medical_accuracy >= 0.6:\n",
    "        print(\"üü° Medical Response Quality: Good\")\n",
    "    else:\n",
    "        print(\"üî¥ Medical Response Quality: Needs Improvement\")\n",
    "    \n",
    "    if ood_accuracy >= 0.8:\n",
    "        print(\"üü¢ Out-of-Domain Handling: Excellent\")\n",
    "    elif ood_accuracy >= 0.6:\n",
    "        print(\"üü° Out-of-Domain Handling: Good\")\n",
    "    else:\n",
    "        print(\"üî¥ Out-of-Domain Handling: Needs Improvement\")\n",
    "    \n",
    "    if 20 <= avg_length <= 80:\n",
    "        print(\"üü¢ Response Length: Appropriate\")\n",
    "    elif avg_length < 20:\n",
    "        print(\"üü° Response Length: Too Short\")\n",
    "    else:\n",
    "        print(\"üü° Response Length: Too Long\")\n",
    "    \n",
    "    # Create summary visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Response type distribution\n",
    "    type_counts = {}\n",
    "    for result in qualitative_results:\n",
    "        response_type = result['type']\n",
    "        type_counts[response_type] = type_counts.get(response_type, 0) + 1\n",
    "    \n",
    "    ax1.pie(type_counts.values(), labels=type_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Response Type Distribution')\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracies = ['Medical Questions', 'Out-of-Domain']\n",
    "    accuracy_values = [medical_accuracy, ood_accuracy]\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    \n",
    "    bars = ax2.bar(accuracies, accuracy_values, color=colors)\n",
    "    ax2.set_title('Response Accuracy')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, value in zip(bars, accuracy_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{value:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No qualitative results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 4: Performance Summary and Recommendations\n",
    "\n",
    "Let's summarize your model's performance and provide improvement suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive performance report\n",
    "if model_loaded:\n",
    "    print(\"üìã COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Collect all metrics\n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'model_path': MODEL_PATH,\n",
    "            'model_name': os.path.basename(os.path.dirname(MODEL_PATH)),\n",
    "            'evaluation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add quantitative metrics if available\n",
    "    if quantitative_completed:\n",
    "        report['quantitative_metrics'] = {\n",
    "            'bleu_score': eval_results.get('corpus_bleu', 0),\n",
    "            'rouge_l_f1': eval_results.get('rougeL_f1', 0),\n",
    "            'semantic_similarity': eval_results.get('semantic_similarity', 0),\n",
    "            'perplexity': eval_results.get('perplexity', 0)\n",
    "        }\n",
    "    \n",
    "    # Add qualitative metrics if available\n",
    "    if qualitative_completed:\n",
    "        report['qualitative_metrics'] = {\n",
    "            'medical_accuracy': medical_accuracy,\n",
    "            'ood_accuracy': ood_accuracy,\n",
    "            'avg_response_length': avg_length,\n",
    "            'overall_accuracy': overall_accuracy\n",
    "        }\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"üèÜ PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    if quantitative_completed:\n",
    "        bleu = eval_results.get('corpus_bleu', 0)\n",
    "        rouge = eval_results.get('rougeL_f1', 0)\n",
    "        similarity = eval_results.get('semantic_similarity', 0)\n",
    "        \n",
    "        print(f\"üìä Quantitative Score: {(bleu + rouge + similarity)/3:.3f}\")\n",
    "        print(f\"   - BLEU Score: {bleu:.3f}\")\n",
    "        print(f\"   - ROUGE-L F1: {rouge:.3f}\")\n",
    "        print(f\"   - Semantic Similarity: {similarity:.3f}\")\n",
    "    \n",
    "    if qualitative_completed:\n",
    "        print(f\"\\nüó£Ô∏è Qualitative Score: {overall_accuracy:.3f}\")\n",
    "        print(f\"   - Medical Question Handling: {medical_accuracy:.1%}\")\n",
    "        print(f\"   - Out-of-Domain Detection: {ood_accuracy:.1%}\")\n",
    "    \n",
    "    # Overall grade\n",
    "    if quantitative_completed and qualitative_completed:\n",
    "        quant_score = (bleu + rouge + similarity) / 3\n",
    "        final_score = (quant_score + overall_accuracy) / 2\n",
    "        \n",
    "        print(f\"\\nüéØ FINAL SCORE: {final_score:.3f}\")\n",
    "        \n",
    "        if final_score >= 0.4:\n",
    "            grade = \"A (Excellent)\"\n",
    "            emoji = \"üèÜ\"\n",
    "        elif final_score >= 0.3:\n",
    "            grade = \"B (Good)\"\n",
    "            emoji = \"üëç\"\n",
    "        elif final_score >= 0.2:\n",
    "            grade = \"C (Fair)\"\n",
    "            emoji = \"üëå\"\n",
    "        else:\n",
    "            grade = \"D (Needs Improvement)\"\n",
    "            emoji = \"üîß\"\n",
    "        \n",
    "        print(f\"üéì GRADE: {grade} {emoji}\")\n",
    "        \n",
    "        report['final_assessment'] = {\n",
    "            'final_score': final_score,\n",
    "            'grade': grade\n",
    "        }\n",
    "    \n",
    "    # Improvement recommendations\n",
    "    print(\"\\nüí° IMPROVEMENT RECOMMENDATIONS:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if quantitative_completed:\n",
    "        if bleu < 0.2:\n",
    "            recommendations.append(\"üîß Low BLEU score - Consider more training epochs or larger model\")\n",
    "        if rouge < 0.3:\n",
    "            recommendations.append(\"üîß Low ROUGE score - Improve answer quality in training data\")\n",
    "        if similarity < 0.25:\n",
    "            recommendations.append(\"üîß Low similarity - Add more diverse training examples\")\n",
    "    \n",
    "    if qualitative_completed:\n",
    "        if medical_accuracy < 0.8:\n",
    "            recommendations.append(\"üîß Medical accuracy low - Fine-tune domain detection\")\n",
    "        if ood_accuracy < 0.8:\n",
    "            recommendations.append(\"üîß Out-of-domain handling poor - Improve rejection mechanism\")\n",
    "        if avg_length < 15:\n",
    "            recommendations.append(\"üîß Responses too short - Encourage longer, more detailed answers\")\n",
    "        elif avg_length > 100:\n",
    "            recommendations.append(\"üîß Responses too long - Encourage more concise answers\")\n",
    "    \n",
    "    # General recommendations\n",
    "    if not recommendations:\n",
    "        recommendations.append(\"üéâ Great job! Your model is performing well.\")\n",
    "        recommendations.append(\"üí° Consider testing with more diverse medical questions\")\n",
    "        recommendations.append(\"üöÄ Ready for deployment!\")\n",
    "    else:\n",
    "        recommendations.extend([\n",
    "            \"üìö Consider adding more training data\",\n",
    "            \"‚öôÔ∏è Try different hyperparameters\",\n",
    "            \"ü§ñ Experiment with different base models\"\n",
    "        ])\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    # Save report\n",
    "    report_path = '/workspace/notebooks/evaluation_report.json'\n",
    "    try:\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        print(f\"\\nüíæ Evaluation report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not save report: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot generate report - model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: Next Steps\n",
    "\n",
    "Congratulations on evaluating your healthcare chatbot! Here's what to do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ EVALUATION COMPLETE!\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if model_loaded:\n",
    "    print(f\"‚úÖ Successfully evaluated your healthcare chatbot!\")\n",
    "    print(f\"ü§ñ Model: {os.path.basename(os.path.dirname(MODEL_PATH))}\")\n",
    "    \n",
    "    print(\"\\nüöÄ NEXT STEPS:\")\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "    print(\"1. üåê Deploy Your Chatbot:\")\n",
    "    print(\"   ‚Üí Open: notebooks/05_Deployment.ipynb\")\n",
    "    print(\"   ‚Üí Launch web interface for others to use\")\n",
    "    \n",
    "    print(\"\\n2. üñ•Ô∏è Quick Testing:\")\n",
    "    print(f\"   ‚Üí CLI: python -m src.chatbot {MODEL_PATH}\")\n",
    "    print(f\"   ‚Üí Web: python -m src.web_interface --model_path {MODEL_PATH}\")\n",
    "    \n",
    "    print(\"\\n3. üîß Model Improvement (if needed):\")\n",
    "    if 'final_score' in locals() and final_score < 0.3:\n",
    "        print(\"   ‚Üí Retrain with more epochs\")\n",
    "        print(\"   ‚Üí Try a larger model (dialogpt-medium)\")\n",
    "        print(\"   ‚Üí Add more training data\")\n",
    "        print(\"   ‚Üí Adjust hyperparameters\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Your model is performing well!\")\n",
    "        print(\"   ‚Üí Consider fine-tuning for specific use cases\")\n",
    "        print(\"   ‚Üí Test with real users for feedback\")\n",
    "    \n",
    "    print(\"\\n4. üìä Share Your Results:\")\n",
    "    print(\"   ‚Üí Show the evaluation visualizations\")\n",
    "    print(\"   ‚Üí Demonstrate the chatbot to others\")\n",
    "    print(\"   ‚Üí Document your findings\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Evaluation was not completed successfully.\")\n",
    "    print(\"\\nüîß TROUBLESHOOTING:\")\n",
    "    print(\"-\"*25)\n",
    "    print(\"1. Make sure you have a trained model\")\n",
    "    print(\"2. Check model path is correct\")\n",
    "    print(\"3. Verify model files exist\")\n",
    "    print(\"4. Try retraining if model is corrupted\")\n",
    "\n",
    "print(\"\\nüìö RESOURCES:\")\n",
    "print(\"-\"*15)\n",
    "print(\"‚Ä¢ Evaluation Report: evaluation_report.json\")\n",
    "print(\"‚Ä¢ Deployment Guide: 05_Deployment.ipynb\")\n",
    "print(\"‚Ä¢ Documentation: README.md\")\n",
    "print(\"‚Ä¢ Improvement Tips: KAGGLE_DATASET_GUIDE.md\")\n",
    "\n",
    "print(\"\\nüè• Your AI healthcare assistant is ready to help patients! ü§ñ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}