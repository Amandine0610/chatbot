{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Healthcare Chatbot - Model Evaluation\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Load your trained healthcare chatbot\n",
    "2. Run comprehensive evaluation metrics\n",
    "3. Test conversational quality\n",
    "4. Analyze performance and get improvement suggestions\n",
    "5. Compare with baseline models\n",
    "\n",
    "Let's see how well your AI doctor performs! ğŸ©ºğŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 1: Setup and Model Loading\n",
    "\n",
    "First, let's set up the environment and load your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('/workspace/src')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"ğŸ”§ Evaluation environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available trained models\n",
    "def find_trained_models():\n",
    "    \"\"\"Find trained model directories.\"\"\"\n",
    "    models_dir = '/workspace/models'\n",
    "    trained_models = []\n",
    "    \n",
    "    if os.path.exists(models_dir):\n",
    "        for item in os.listdir(models_dir):\n",
    "            model_path = os.path.join(models_dir, item)\n",
    "            if os.path.isdir(model_path):\n",
    "                # Look for final_model subdirectory\n",
    "                final_model_path = os.path.join(model_path, 'final_model')\n",
    "                if os.path.exists(final_model_path):\n",
    "                    trained_models.append(final_model_path)\n",
    "                # Or check if it's directly a model directory\n",
    "                elif any(f.endswith('.bin') or f.endswith('.safetensors') for f in os.listdir(model_path)):\n",
    "                    trained_models.append(model_path)\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "print(\"ğŸ” Searching for trained models...\")\n",
    "available_models = find_trained_models()\n",
    "\n",
    "if available_models:\n",
    "    print(f\"ğŸ“ Found {len(available_models)} trained model(s):\")\n",
    "    for i, model_path in enumerate(available_models, 1):\n",
    "        # Get model info\n",
    "        model_name = os.path.basename(os.path.dirname(model_path)) if model_path.endswith('final_model') else os.path.basename(model_path)\n",
    "        \n",
    "        # Check for training summary\n",
    "        summary_path = os.path.join(os.path.dirname(model_path), 'kaggle_training_summary.json')\n",
    "        if not os.path.exists(summary_path):\n",
    "            summary_path = os.path.join(os.path.dirname(model_path), 'training_summary.json')\n",
    "        \n",
    "        timestamp = \"Unknown\"\n",
    "        if os.path.exists(summary_path):\n",
    "            try:\n",
    "                with open(summary_path, 'r') as f:\n",
    "                    summary = json.load(f)\n",
    "                timestamp = summary.get('timestamp', 'Unknown')\n",
    "                if timestamp != 'Unknown':\n",
    "                    timestamp = datetime.fromisoformat(timestamp).strftime('%Y-%m-%d %H:%M')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"   {i}. {model_name} (trained: {timestamp})\")\n",
    "        print(f\"      Path: {model_path}\")\n",
    "else:\n",
    "    print(\"âŒ No trained models found.\")\n",
    "    print(\"\\nğŸ”§ SOLUTIONS:\")\n",
    "    print(\"1. Train a model first using notebook 03_Model_Training.ipynb\")\n",
    "    print(\"2. Or run: python train_kaggle_chatbot.py --kaggle_dataset_path your-dataset.csv\")\n",
    "    print(\"3. Check that training completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model for evaluation\n",
    "# MODIFY THIS: Set the path to your trained model\n",
    "MODEL_PATH = None\n",
    "\n",
    "# Auto-select the most recent model if available\n",
    "if available_models:\n",
    "    MODEL_PATH = available_models[0]  # Use the first (most recent) model\n",
    "    print(f\"ğŸ¤– Auto-selected model: {MODEL_PATH}\")\n",
    "\n",
    "# Or manually set the path\n",
    "# MODEL_PATH = '/workspace/models/my_healthcare_chatbot/final_model'\n",
    "\n",
    "if MODEL_PATH and os.path.exists(MODEL_PATH):\n",
    "    print(f\"âœ… Model selected for evaluation: {os.path.basename(os.path.dirname(MODEL_PATH))}\")\n",
    "    \n",
    "    # Load model info if available\n",
    "    model_info = {}\n",
    "    summary_path = os.path.join(os.path.dirname(MODEL_PATH), 'kaggle_training_summary.json')\n",
    "    if not os.path.exists(summary_path):\n",
    "        summary_path = os.path.join(os.path.dirname(MODEL_PATH), 'training_summary.json')\n",
    "    \n",
    "    if os.path.exists(summary_path):\n",
    "        with open(summary_path, 'r') as f:\n",
    "            model_info = json.load(f)\n",
    "        print(\"ğŸ“‹ Training information loaded\")\n",
    "    \n",
    "    model_selected = True\n",
    "else:\n",
    "    print(\"âŒ Please set MODEL_PATH to your trained model directory.\")\n",
    "    print(\"   Example: MODEL_PATH = '/workspace/models/my_healthcare_chatbot/final_model'\")\n",
    "    model_selected = False\n",
    "    model_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "if model_selected:\n",
    "    print(\"ğŸ”„ LOADING TRAINED MODEL\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    try:\n",
    "        from evaluation import HealthcareEvaluator\n",
    "        from chatbot import HealthcareChatbot\n",
    "        \n",
    "        # Initialize evaluator\n",
    "        print(\"ğŸ”„ Initializing evaluator...\")\n",
    "        evaluator = HealthcareEvaluator(MODEL_PATH)\n",
    "        \n",
    "        # Initialize chatbot for interactive testing\n",
    "        print(\"ğŸ”„ Initializing chatbot...\")\n",
    "        chatbot = HealthcareChatbot(MODEL_PATH)\n",
    "        \n",
    "        print(\"âœ… Model loaded successfully!\")\n",
    "        \n",
    "        # Display model information\n",
    "        if model_info:\n",
    "            print(\"\\nğŸ“Š Model Information:\")\n",
    "            training_config = model_info.get('training_config', {})\n",
    "            if training_config:\n",
    "                print(f\"   Model type: {training_config.get('model_key', 'Unknown')}\")\n",
    "                print(f\"   Training samples: {model_info.get('dataset_stats', {}).get('training_samples', 'Unknown')}\")\n",
    "                print(f\"   Validation samples: {model_info.get('dataset_stats', {}).get('validation_samples', 'Unknown')}\")\n",
    "                \n",
    "                hyperparams = training_config.get('hyperparameters', {})\n",
    "                if hyperparams:\n",
    "                    print(f\"   Learning rate: {hyperparams.get('learning_rate', 'Unknown')}\")\n",
    "                    print(f\"   Epochs: {hyperparams.get('num_train_epochs', 'Unknown')}\")\n",
    "                    print(f\"   Batch size: {hyperparams.get('per_device_train_batch_size', 'Unknown')}\")\n",
    "        \n",
    "        model_loaded = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading model: {e}\")\n",
    "        print(\"\\nğŸ”§ TROUBLESHOOTING:\")\n",
    "        print(\"1. Check that the model path is correct\")\n",
    "        print(\"2. Ensure training completed successfully\")\n",
    "        print(\"3. Verify model files exist in the directory\")\n",
    "        \n",
    "        model_loaded = False\n",
    "        evaluator = None\n",
    "        chatbot = None\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ Cannot load model - no model selected\")\n",
    "    model_loaded = False\n",
    "    evaluator = None\n",
    "    chatbot = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 2: Quantitative Evaluation\n",
    "\n",
    "Let's run comprehensive metrics on your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation dataset\n",
    "if model_loaded:\n",
    "    print(\"ğŸ“Š PREPARING EVALUATION DATASET\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Try to find the dataset used for training\n",
    "    dataset_paths = [\n",
    "        '/workspace/data/kaggle_medical_dataset.json',\n",
    "        '/workspace/data/healthcare_qa_dataset.json'\n",
    "    ]\n",
    "    \n",
    "    eval_dataset = None\n",
    "    dataset_path = None\n",
    "    \n",
    "    for path in dataset_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    eval_dataset = json.load(f)\n",
    "                dataset_path = path\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if eval_dataset:\n",
    "        print(f\"âœ… Evaluation dataset loaded: {os.path.basename(dataset_path)}\")\n",
    "        print(f\"ğŸ“Š Total samples: {len(eval_dataset)}\")\n",
    "        \n",
    "        # Limit samples for faster evaluation if dataset is large\n",
    "        max_eval_samples = 100\n",
    "        if len(eval_dataset) > max_eval_samples:\n",
    "            eval_dataset = eval_dataset[:max_eval_samples]\n",
    "            print(f\"ğŸ“Š Limited to {max_eval_samples} samples for faster evaluation\")\n",
    "        \n",
    "        dataset_ready = True\n",
    "    else:\n",
    "        print(\"âŒ No evaluation dataset found\")\n",
    "        print(\"\\nğŸ”§ SOLUTIONS:\")\n",
    "        print(\"1. Make sure you've processed your dataset (notebook 02)\")\n",
    "        print(\"2. Check dataset paths above\")\n",
    "        print(\"3. You can still do qualitative evaluation below\")\n",
    "        \n",
    "        dataset_ready = False\n",
    "        eval_dataset = None\n",
    "else:\n",
    "    print(\"âš ï¸ Model not loaded - cannot prepare evaluation\")\n",
    "    dataset_ready = False\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quantitative evaluation\n",
    "if model_loaded and dataset_ready:\n",
    "    print(\"ğŸ§ª RUNNING QUANTITATIVE EVALUATION\")\n",
    "    print(\"=\"*45)\n",
    "    print(\"â±ï¸ This may take a few minutes...\")\n",
    "    \n",
    "    try:\n",
    "        # Save dataset temporarily for evaluator\n",
    "        temp_dataset_path = '/tmp/eval_dataset.json'\n",
    "        with open(temp_dataset_path, 'w') as f:\n",
    "            json.dump(eval_dataset, f)\n",
    "        \n",
    "        # Run evaluation\n",
    "        eval_results = evaluator.evaluate_on_dataset(\n",
    "            temp_dataset_path,\n",
    "            num_samples=len(eval_dataset)\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… EVALUATION COMPLETED!\")\n",
    "        print(\"=\"*30)\n",
    "        \n",
    "        # Display key metrics\n",
    "        metrics = {\n",
    "            'BLEU Score': eval_results.get('corpus_bleu', 0),\n",
    "            'ROUGE-1 F1': eval_results.get('rouge1_f1', 0),\n",
    "            'ROUGE-2 F1': eval_results.get('rouge2_f1', 0),\n",
    "            'ROUGE-L F1': eval_results.get('rougeL_f1', 0),\n",
    "            'Semantic Similarity': eval_results.get('semantic_similarity', 0),\n",
    "            'Perplexity': eval_results.get('perplexity', 0),\n",
    "            'Avg Prediction Length': eval_results.get('avg_prediction_length', 0),\n",
    "            'Avg Reference Length': eval_results.get('avg_reference_length', 0)\n",
    "        }\n",
    "        \n",
    "        print(\"ğŸ“Š PERFORMANCE METRICS:\")\n",
    "        print(\"-\"*30)\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"{metric:.<25} {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric:.<25} {value}\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        print(\"\\nğŸ¯ PERFORMANCE ASSESSMENT:\")\n",
    "        print(\"-\"*35)\n",
    "        \n",
    "        bleu_score = eval_results.get('corpus_bleu', 0)\n",
    "        rouge_l = eval_results.get('rougeL_f1', 0)\n",
    "        similarity = eval_results.get('semantic_similarity', 0)\n",
    "        \n",
    "        assessments = []\n",
    "        \n",
    "        # BLEU assessment\n",
    "        if bleu_score > 0.3:\n",
    "            assessments.append(\"ğŸŸ¢ BLEU: Excellent (>0.30)\")\n",
    "        elif bleu_score > 0.2:\n",
    "            assessments.append(\"ğŸŸ¡ BLEU: Good (0.20-0.30)\")\n",
    "        elif bleu_score > 0.1:\n",
    "            assessments.append(\"ğŸŸ  BLEU: Fair (0.10-0.20)\")\n",
    "        else:\n",
    "            assessments.append(\"ğŸ”´ BLEU: Needs improvement (<0.10)\")\n",
    "        \n",
    "        # ROUGE assessment\n",
    "        if rouge_l > 0.4:\n",
    "            assessments.append(\"ğŸŸ¢ ROUGE-L: Excellent (>0.40)\")\n",
    "        elif rouge_l > 0.3:\n",
    "            assessments.append(\"ğŸŸ¡ ROUGE-L: Good (0.30-0.40)\")\n",
    "        elif rouge_l > 0.2:\n",
    "            assessments.append(\"ğŸŸ  ROUGE-L: Fair (0.20-0.30)\")\n",
    "        else:\n",
    "            assessments.append(\"ğŸ”´ ROUGE-L: Needs improvement (<0.20)\")\n",
    "        \n",
    "        # Similarity assessment\n",
    "        if similarity > 0.4:\n",
    "            assessments.append(\"ğŸŸ¢ Similarity: Excellent (>0.40)\")\n",
    "        elif similarity > 0.3:\n",
    "            assessments.append(\"ğŸŸ¡ Similarity: Good (0.30-0.40)\")\n",
    "        elif similarity > 0.2:\n",
    "            assessments.append(\"ğŸŸ  Similarity: Fair (0.20-0.30)\")\n",
    "        else:\n",
    "            assessments.append(\"ğŸ”´ Similarity: Needs improvement (<0.20)\")\n",
    "        \n",
    "        for assessment in assessments:\n",
    "            print(assessment)\n",
    "        \n",
    "        # Overall score\n",
    "        overall_score = (bleu_score + rouge_l + similarity) / 3\n",
    "        print(f\"\\nğŸ“Š Overall Score: {overall_score:.3f}\")\n",
    "        \n",
    "        if overall_score > 0.35:\n",
    "            print(\"ğŸ‰ Excellent performance! Your chatbot is working very well.\")\n",
    "        elif overall_score > 0.25:\n",
    "            print(\"ğŸ‘ Good performance! Your chatbot is functioning well.\")\n",
    "        elif overall_score > 0.15:\n",
    "            print(\"ğŸ‘Œ Fair performance. Consider improvements.\")\n",
    "        else:\n",
    "            print(\"ğŸ”§ Performance needs improvement. See suggestions below.\")\n",
    "        \n",
    "        quantitative_completed = True\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(temp_dataset_path):\n",
    "            os.remove(temp_dataset_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during evaluation: {e}\")\n",
    "        print(\"\\nğŸ”§ TROUBLESHOOTING:\")\n",
    "        print(\"1. Check that the model is properly loaded\")\n",
    "        print(\"2. Reduce evaluation dataset size\")\n",
    "        print(\"3. Check available memory\")\n",
    "        \n",
    "        quantitative_completed = False\n",
    "        eval_results = {}\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ Cannot run quantitative evaluation - prerequisites not met\")\n",
    "    quantitative_completed = False\n",
    "    eval_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "if quantitative_completed and eval_results:\n",
    "    print(\"ğŸ“Š CREATING EVALUATION VISUALIZATIONS\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Create metrics visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Healthcare Chatbot Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Main metrics bar chart\n",
    "    main_metrics = {\n",
    "        'BLEU': eval_results.get('corpus_bleu', 0),\n",
    "        'ROUGE-L': eval_results.get('rougeL_f1', 0),\n",
    "        'Similarity': eval_results.get('semantic_similarity', 0)\n",
    "    }\n",
    "    \n",
    "    bars = axes[0, 0].bar(main_metrics.keys(), main_metrics.values(), \n",
    "                         color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "    axes[0, 0].set_title('Main Performance Metrics')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, main_metrics.values()):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. ROUGE metrics breakdown\n",
    "    rouge_metrics = {\n",
    "        'ROUGE-1': eval_results.get('rouge1_f1', 0),\n",
    "        'ROUGE-2': eval_results.get('rouge2_f1', 0),\n",
    "        'ROUGE-L': eval_results.get('rougeL_f1', 0)\n",
    "    }\n",
    "    \n",
    "    bars = axes[0, 1].bar(rouge_metrics.keys(), rouge_metrics.values(), \n",
    "                         color=['orange', 'yellow', 'lightgreen'])\n",
    "    axes[0, 1].set_title('ROUGE Metrics Breakdown')\n",
    "    axes[0, 1].set_ylabel('F1 Score')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    for bar, value in zip(bars, rouge_metrics.values()):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Response length comparison\n",
    "    pred_length = eval_results.get('avg_prediction_length', 0)\n",
    "    ref_length = eval_results.get('avg_reference_length', 0)\n",
    "    \n",
    "    length_data = ['Predictions', 'References']\n",
    "    length_values = [pred_length, ref_length]\n",
    "    \n",
    "    bars = axes[1, 0].bar(length_data, length_values, color=['purple', 'brown'])\n",
    "    axes[1, 0].set_title('Average Response Length')\n",
    "    axes[1, 0].set_ylabel('Words')\n",
    "    \n",
    "    for bar, value in zip(bars, length_values):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                       f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Performance radar chart (simplified as bar chart)\n",
    "    performance_categories = {\n",
    "        'Accuracy': min(eval_results.get('corpus_bleu', 0) * 2, 1),  # Scale BLEU\n",
    "        'Fluency': min(eval_results.get('rougeL_f1', 0) * 1.5, 1),   # Scale ROUGE-L\n",
    "        'Relevance': eval_results.get('semantic_similarity', 0),\n",
    "        'Length': min(abs(1 - eval_results.get('length_ratio', 1)), 1)  # Length appropriateness\n",
    "    }\n",
    "    \n",
    "    bars = axes[1, 1].bar(performance_categories.keys(), performance_categories.values(),\n",
    "                         color=['red', 'blue', 'green', 'orange'])\n",
    "    axes[1, 1].set_title('Performance Categories')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, performance_categories.values()):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Visualizations created!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No evaluation results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—£ï¸ Step 3: Qualitative Evaluation\n",
    "\n",
    "Let's test your chatbot with real conversations and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test questions\n",
    "if model_loaded:\n",
    "    print(\"ğŸ—£ï¸ QUALITATIVE EVALUATION\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Comprehensive test questions covering different medical areas\n",
    "    test_questions = [\n",
    "        # Basic symptoms\n",
    "        \"What are the symptoms of diabetes?\",\n",
    "        \"How do I know if I have high blood pressure?\",\n",
    "        \"What causes chest pain?\",\n",
    "        \n",
    "        # Treatment and medication\n",
    "        \"How is pneumonia treated?\",\n",
    "        \"What are the side effects of aspirin?\",\n",
    "        \"How long should I take antibiotics?\",\n",
    "        \n",
    "        # Prevention and wellness\n",
    "        \"How can I prevent heart disease?\",\n",
    "        \"What foods boost immunity?\",\n",
    "        \"How much exercise do I need daily?\",\n",
    "        \n",
    "        # Emergency situations\n",
    "        \"What should I do if someone has a heart attack?\",\n",
    "        \"When should I go to the emergency room?\",\n",
    "        \n",
    "        # Out-of-domain tests\n",
    "        \"What's the weather like today?\",\n",
    "        \"Can you help me with my math homework?\",\n",
    "        \"What's the capital of France?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ§ª Testing with {len(test_questions)} questions...\")\n",
    "    print(\"\\nğŸ¤– CHATBOT RESPONSES:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    qualitative_results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i:2d}. ğŸ‘¤ User: {question}\")\n",
    "        \n",
    "        try:\n",
    "            response = chatbot.chat(question)\n",
    "            answer = response['response']\n",
    "            response_type = response['type']\n",
    "            \n",
    "            # Determine if this is a healthcare question\n",
    "            is_medical = response_type == 'healthcare'\n",
    "            \n",
    "            # Color coding for response types\n",
    "            if response_type == 'healthcare':\n",
    "                icon = \"ğŸ¥\"\n",
    "            elif response_type == 'out_of_domain':\n",
    "                icon = \"âš ï¸\"\n",
    "            else:\n",
    "                icon = \"ğŸ’¬\"\n",
    "            \n",
    "            print(f\"    {icon} Bot: {answer}\")\n",
    "            print(f\"    ğŸ“Š Type: {response_type}\")\n",
    "            \n",
    "            # Store result for analysis\n",
    "            qualitative_results.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'type': response_type,\n",
    "                'is_medical': is_medical,\n",
    "                'is_out_of_domain': 'weather' in question.lower() or 'math' in question.lower() or 'capital' in question.lower()\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Error: {e}\")\n",
    "            qualitative_results.append({\n",
    "                'question': question,\n",
    "                'answer': f\"Error: {e}\",\n",
    "                'type': 'error',\n",
    "                'is_medical': False,\n",
    "                'is_out_of_domain': False\n",
    "            })\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    qualitative_completed = True\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Cannot run qualitative evaluation - model not loaded\")\n",
    "    qualitative_completed = False\n",
    "    qualitative_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze qualitative results\n",
    "if qualitative_completed and qualitative_results:\n",
    "    print(\"ğŸ“Š QUALITATIVE ANALYSIS\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Calculate response type accuracy\n",
    "    medical_questions = [r for r in qualitative_results if not r['is_out_of_domain'] and r['type'] != 'error']\n",
    "    out_of_domain_questions = [r for r in qualitative_results if r['is_out_of_domain']]\n",
    "    \n",
    "    # Medical question handling\n",
    "    medical_correct = sum(1 for r in medical_questions if r['is_medical'])\n",
    "    medical_total = len(medical_questions)\n",
    "    medical_accuracy = medical_correct / medical_total if medical_total > 0 else 0\n",
    "    \n",
    "    # Out-of-domain handling\n",
    "    ood_correct = sum(1 for r in out_of_domain_questions if not r['is_medical'])\n",
    "    ood_total = len(out_of_domain_questions)\n",
    "    ood_accuracy = ood_correct / ood_total if ood_total > 0 else 0\n",
    "    \n",
    "    # Response length analysis\n",
    "    response_lengths = [len(r['answer'].split()) for r in qualitative_results if r['type'] != 'error']\n",
    "    avg_length = np.mean(response_lengths) if response_lengths else 0\n",
    "    \n",
    "    print(\"ğŸ“ˆ RESPONSE ANALYSIS:\")\n",
    "    print(\"-\"*25)\n",
    "    print(f\"Medical Question Accuracy: {medical_accuracy:.1%} ({medical_correct}/{medical_total})\")\n",
    "    print(f\"Out-of-Domain Handling: {ood_accuracy:.1%} ({ood_correct}/{ood_total})\")\n",
    "    print(f\"Average Response Length: {avg_length:.1f} words\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    overall_accuracy = (medical_accuracy + ood_accuracy) / 2\n",
    "    print(f\"\\nğŸ¯ Overall Conversational Accuracy: {overall_accuracy:.1%}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(\"\\nğŸ” QUALITY ASSESSMENT:\")\n",
    "    print(\"-\"*25)\n",
    "    \n",
    "    if medical_accuracy >= 0.8:\n",
    "        print(\"ğŸŸ¢ Medical Response Quality: Excellent\")\n",
    "    elif medical_accuracy >= 0.6:\n",
    "        print(\"ğŸŸ¡ Medical Response Quality: Good\")\n",
    "    else:\n",
    "        print(\"ğŸ”´ Medical Response Quality: Needs Improvement\")\n",
    "    \n",
    "    if ood_accuracy >= 0.8:\n",
    "        print(\"ğŸŸ¢ Out-of-Domain Handling: Excellent\")\n",
    "    elif ood_accuracy >= 0.6:\n",
    "        print(\"ğŸŸ¡ Out-of-Domain Handling: Good\")\n",
    "    else:\n",
    "        print(\"ğŸ”´ Out-of-Domain Handling: Needs Improvement\")\n",
    "    \n",
    "    if 20 <= avg_length <= 80:\n",
    "        print(\"ğŸŸ¢ Response Length: Appropriate\")\n",
    "    elif avg_length < 20:\n",
    "        print(\"ğŸŸ¡ Response Length: Too Short\")\n",
    "    else:\n",
    "        print(\"ğŸŸ¡ Response Length: Too Long\")\n",
    "    \n",
    "    # Create summary visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Response type distribution\n",
    "    type_counts = {}\n",
    "    for result in qualitative_results:\n",
    "        response_type = result['type']\n",
    "        type_counts[response_type] = type_counts.get(response_type, 0) + 1\n",
    "    \n",
    "    ax1.pie(type_counts.values(), labels=type_counts.keys(), autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Response Type Distribution')\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    accuracies = ['Medical Questions', 'Out-of-Domain']\n",
    "    accuracy_values = [medical_accuracy, ood_accuracy]\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    \n",
    "    bars = ax2.bar(accuracies, accuracy_values, color=colors)\n",
    "    ax2.set_title('Response Accuracy')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar, value in zip(bars, accuracy_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{value:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No qualitative results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 4: Performance Summary and Recommendations\n",
    "\n",
    "Let's summarize your model's performance and provide improvement suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive performance report\n",
    "if model_loaded:\n",
    "    print(\"ğŸ“‹ COMPREHENSIVE PERFORMANCE REPORT\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Collect all metrics\n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'model_path': MODEL_PATH,\n",
    "            'model_name': os.path.basename(os.path.dirname(MODEL_PATH)),\n",
    "            'evaluation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add quantitative metrics if available\n",
    "    if quantitative_completed:\n",
    "        report['quantitative_metrics'] = {\n",
    "            'bleu_score': eval_results.get('corpus_bleu', 0),\n",
    "            'rouge_l_f1': eval_results.get('rougeL_f1', 0),\n",
    "            'semantic_similarity': eval_results.get('semantic_similarity', 0),\n",
    "            'perplexity': eval_results.get('perplexity', 0)\n",
    "        }\n",
    "    \n",
    "    # Add qualitative metrics if available\n",
    "    if qualitative_completed:\n",
    "        report['qualitative_metrics'] = {\n",
    "            'medical_accuracy': medical_accuracy,\n",
    "            'ood_accuracy': ood_accuracy,\n",
    "            'avg_response_length': avg_length,\n",
    "            'overall_accuracy': overall_accuracy\n",
    "        }\n",
    "    \n",
    "    # Performance summary\n",
    "    print(\"ğŸ† PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    if quantitative_completed:\n",
    "        bleu = eval_results.get('corpus_bleu', 0)\n",
    "        rouge = eval_results.get('rougeL_f1', 0)\n",
    "        similarity = eval_results.get('semantic_similarity', 0)\n",
    "        \n",
    "        print(f\"ğŸ“Š Quantitative Score: {(bleu + rouge + similarity)/3:.3f}\")\n",
    "        print(f\"   - BLEU Score: {bleu:.3f}\")\n",
    "        print(f\"   - ROUGE-L F1: {rouge:.3f}\")\n",
    "        print(f\"   - Semantic Similarity: {similarity:.3f}\")\n",
    "    \n",
    "    if qualitative_completed:\n",
    "        print(f\"\\nğŸ—£ï¸ Qualitative Score: {overall_accuracy:.3f}\")\n",
    "        print(f\"   - Medical Question Handling: {medical_accuracy:.1%}\")\n",
    "        print(f\"   - Out-of-Domain Detection: {ood_accuracy:.1%}\")\n",
    "    \n",
    "    # Overall grade\n",
    "    if quantitative_completed and qualitative_completed:\n",
    "        quant_score = (bleu + rouge + similarity) / 3\n",
    "        final_score = (quant_score + overall_accuracy) / 2\n",
    "        \n",
    "        print(f\"\\nğŸ¯ FINAL SCORE: {final_score:.3f}\")\n",
    "        \n",
    "        if final_score >= 0.4:\n",
    "            grade = \"A (Excellent)\"\n",
    "            emoji = \"ğŸ†\"\n",
    "        elif final_score >= 0.3:\n",
    "            grade = \"B (Good)\"\n",
    "            emoji = \"ğŸ‘\"\n",
    "        elif final_score >= 0.2:\n",
    "            grade = \"C (Fair)\"\n",
    "            emoji = \"ğŸ‘Œ\"\n",
    "        else:\n",
    "            grade = \"D (Needs Improvement)\"\n",
    "            emoji = \"ğŸ”§\"\n",
    "        \n",
    "        print(f\"ğŸ“ GRADE: {grade} {emoji}\")\n",
    "        \n",
    "        report['final_assessment'] = {\n",
    "            'final_score': final_score,\n",
    "            'grade': grade\n",
    "        }\n",
    "    \n",
    "    # Improvement recommendations\n",
    "    print(\"\\nğŸ’¡ IMPROVEMENT RECOMMENDATIONS:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if quantitative_completed:\n",
    "        if bleu < 0.2:\n",
    "            recommendations.append(\"ğŸ”§ Low BLEU score - Consider more training epochs or larger model\")\n",
    "        if rouge < 0.3:\n",
    "            recommendations.append(\"ğŸ”§ Low ROUGE score - Improve answer quality in training data\")\n",
    "        if similarity < 0.25:\n",
    "            recommendations.append(\"ğŸ”§ Low similarity - Add more diverse training examples\")\n",
    "    \n",
    "    if qualitative_completed:\n",
    "        if medical_accuracy < 0.8:\n",
    "            recommendations.append(\"ğŸ”§ Medical accuracy low - Fine-tune domain detection\")\n",
    "        if ood_accuracy < 0.8:\n",
    "            recommendations.append(\"ğŸ”§ Out-of-domain handling poor - Improve rejection mechanism\")\n",
    "        if avg_length < 15:\n",
    "            recommendations.append(\"ğŸ”§ Responses too short - Encourage longer, more detailed answers\")\n",
    "        elif avg_length > 100:\n",
    "            recommendations.append(\"ğŸ”§ Responses too long - Encourage more concise answers\")\n",
    "    \n",
    "    # General recommendations\n",
    "    if not recommendations:\n",
    "        recommendations.append(\"ğŸ‰ Great job! Your model is performing well.\")\n",
    "        recommendations.append(\"ğŸ’¡ Consider testing with more diverse medical questions\")\n",
    "        recommendations.append(\"ğŸš€ Ready for deployment!\")\n",
    "    else:\n",
    "        recommendations.extend([\n",
    "            \"ğŸ“š Consider adding more training data\",\n",
    "            \"âš™ï¸ Try different hyperparameters\",\n",
    "            \"ğŸ¤– Experiment with different base models\"\n",
    "        ])\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    # Save report\n",
    "    report_path = '/workspace/notebooks/evaluation_report.json'\n",
    "    try:\n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        print(f\"\\nğŸ’¾ Evaluation report saved to: {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not save report: {e}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Cannot generate report - model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 5: Next Steps\n",
    "\n",
    "Congratulations on evaluating your healthcare chatbot! Here's what to do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰ EVALUATION COMPLETE!\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if model_loaded:\n",
    "    print(f\"âœ… Successfully evaluated your healthcare chatbot!\")\n",
    "    print(f\"ğŸ¤– Model: {os.path.basename(os.path.dirname(MODEL_PATH))}\")\n",
    "    \n",
    "    print(\"\\nğŸš€ NEXT STEPS:\")\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "    print(\"1. ğŸŒ Deploy Your Chatbot:\")\n",
    "    print(\"   â†’ Open: notebooks/05_Deployment.ipynb\")\n",
    "    print(\"   â†’ Launch web interface for others to use\")\n",
    "    \n",
    "    print(\"\\n2. ğŸ–¥ï¸ Quick Testing:\")\n",
    "    print(f\"   â†’ CLI: python -m src.chatbot {MODEL_PATH}\")\n",
    "    print(f\"   â†’ Web: python -m src.web_interface --model_path {MODEL_PATH}\")\n",
    "    \n",
    "    print(\"\\n3. ğŸ”§ Model Improvement (if needed):\")\n",
    "    if 'final_score' in locals() and final_score < 0.3:\n",
    "        print(\"   â†’ Retrain with more epochs\")\n",
    "        print(\"   â†’ Try a larger model (dialogpt-medium)\")\n",
    "        print(\"   â†’ Add more training data\")\n",
    "        print(\"   â†’ Adjust hyperparameters\")\n",
    "    else:\n",
    "        print(\"   â†’ Your model is performing well!\")\n",
    "        print(\"   â†’ Consider fine-tuning for specific use cases\")\n",
    "        print(\"   â†’ Test with real users for feedback\")\n",
    "    \n",
    "    print(\"\\n4. ğŸ“Š Share Your Results:\")\n",
    "    print(\"   â†’ Show the evaluation visualizations\")\n",
    "    print(\"   â†’ Demonstrate the chatbot to others\")\n",
    "    print(\"   â†’ Document your findings\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Evaluation was not completed successfully.\")\n",
    "    print(\"\\nğŸ”§ TROUBLESHOOTING:\")\n",
    "    print(\"-\"*25)\n",
    "    print(\"1. Make sure you have a trained model\")\n",
    "    print(\"2. Check model path is correct\")\n",
    "    print(\"3. Verify model files exist\")\n",
    "    print(\"4. Try retraining if model is corrupted\")\n",
    "\n",
    "print(\"\\nğŸ“š RESOURCES:\")\n",
    "print(\"-\"*15)\n",
    "print(\"â€¢ Evaluation Report: evaluation_report.json\")\n",
    "print(\"â€¢ Deployment Guide: 05_Deployment.ipynb\")\n",
    "print(\"â€¢ Documentation: README.md\")\n",
    "print(\"â€¢ Improvement Tips: KAGGLE_DATASET_GUIDE.md\")\n",
    "\n",
    "print(\"\\nğŸ¥ Your AI healthcare assistant is ready to help patients! ğŸ¤–\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}