{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Healthcare Chatbot - Data Exploration\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Upload and explore your Kaggle ai-medical-chatbot dataset\n",
    "2. Analyze data quality and statistics\n",
    "3. Visualize the data distribution\n",
    "4. Prepare data for training\n",
    "\n",
    "Let's dive into your medical data! üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: Dataset Upload and Detection\n",
    "\n",
    "First, let's help you upload and detect your Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('/workspace/src')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üîß Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for existing datasets\n",
    "def find_datasets():\n",
    "    \"\"\"Find potential dataset files in the workspace.\"\"\"\n",
    "    workspace = Path('/workspace')\n",
    "    \n",
    "    # Common dataset file patterns\n",
    "    patterns = ['*.csv', '*.json', '*.jsonl']\n",
    "    datasets = []\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        datasets.extend(workspace.glob(pattern))\n",
    "        datasets.extend(workspace.glob(f'**/{pattern}'))\n",
    "    \n",
    "    # Filter out known non-dataset files\n",
    "    exclude = ['requirements.txt', 'package.json', 'test_']\n",
    "    datasets = [d for d in datasets if not any(ex in d.name for ex in exclude)]\n",
    "    \n",
    "    return sorted(set(datasets))\n",
    "\n",
    "print(\"üîç Searching for datasets in workspace...\")\n",
    "found_datasets = find_datasets()\n",
    "\n",
    "if found_datasets:\n",
    "    print(f\"üìÅ Found {len(found_datasets)} potential dataset files:\")\n",
    "    for i, dataset in enumerate(found_datasets, 1):\n",
    "        size_mb = dataset.stat().st_size / (1024 * 1024)\n",
    "        print(f\"   {i}. {dataset.name} ({size_mb:.1f} MB) - {dataset.parent}\")\n",
    "    \n",
    "    print(\"\\nüí° If one of these is your Kaggle dataset, note the number for the next step.\")\n",
    "else:\n",
    "    print(\"üìÇ No datasets found in workspace.\")\n",
    "    print(\"\\nüì§ Please upload your Kaggle ai-medical-chatbot dataset file to the workspace.\")\n",
    "    print(\"   Common names: ai-medical-chatbot.csv, medical_data.csv, healthcare_qa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your dataset\n",
    "# MODIFY THIS: Set the path to your Kaggle dataset\n",
    "DATASET_PATH = None  # e.g., '/workspace/ai-medical-chatbot.csv'\n",
    "\n",
    "# If you found datasets above, you can select one:\n",
    "if found_datasets:\n",
    "    print(\"üìã Available datasets:\")\n",
    "    for i, dataset in enumerate(found_datasets, 1):\n",
    "        print(f\"   {i}. {dataset}\")\n",
    "    \n",
    "    # Uncomment and modify the line below to select a dataset by number\n",
    "    # DATASET_PATH = str(found_datasets[0])  # Change 0 to your dataset number - 1\n",
    "\n",
    "# Or set the path directly\n",
    "# DATASET_PATH = '/workspace/your-dataset-file.csv'\n",
    "\n",
    "if DATASET_PATH and os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ Dataset selected: {DATASET_PATH}\")\n",
    "    print(f\"üìä File size: {os.path.getsize(DATASET_PATH) / (1024*1024):.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå Please set DATASET_PATH to your Kaggle dataset file.\")\n",
    "    print(\"   Modify the DATASET_PATH variable in the cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2: Load and Analyze Your Dataset\n",
    "\n",
    "Now let's load your dataset and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using our Kaggle loader\n",
    "if DATASET_PATH and os.path.exists(DATASET_PATH):\n",
    "    from kaggle_data_loader import KaggleMedicalDataLoader\n",
    "    \n",
    "    print(f\"üìÇ Loading dataset: {os.path.basename(DATASET_PATH)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Initialize loader\n",
    "        loader = KaggleMedicalDataLoader(DATASET_PATH)\n",
    "        \n",
    "        # Detect format\n",
    "        format_type = loader.detect_format()\n",
    "        print(f\"üìÅ Detected format: {format_type.upper()}\")\n",
    "        \n",
    "        # Load raw data\n",
    "        raw_data = loader.load_data()\n",
    "        print(f\"‚úÖ Raw data loaded successfully!\")\n",
    "        \n",
    "        # Show raw data info\n",
    "        if isinstance(raw_data, pd.DataFrame):\n",
    "            print(f\"üìã DataFrame shape: {raw_data.shape}\")\n",
    "            print(f\"üìã Columns: {list(raw_data.columns)}\")\n",
    "            print(f\"üìã Data types:\")\n",
    "            for col, dtype in raw_data.dtypes.items():\n",
    "                print(f\"   {col}: {dtype}\")\n",
    "        elif isinstance(raw_data, list):\n",
    "            print(f\"üìã List with {len(raw_data)} items\")\n",
    "            if raw_data:\n",
    "                print(f\"üìã Sample keys: {list(raw_data[0].keys())}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        raw_data = None\n",
    "        loader = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please set DATASET_PATH first in the cell above.\")\n",
    "    raw_data = None\n",
    "    loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview raw data\n",
    "if raw_data is not None:\n",
    "    print(\"üëÄ RAW DATA PREVIEW\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    if isinstance(raw_data, pd.DataFrame):\n",
    "        print(\"üìä First 3 rows:\")\n",
    "        display(raw_data.head(3))\n",
    "        \n",
    "        print(\"\\nüìä Data info:\")\n",
    "        print(raw_data.info())\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = raw_data.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\n‚ö†Ô∏è Missing values:\")\n",
    "            for col, count in missing[missing > 0].items():\n",
    "                print(f\"   {col}: {count} ({count/len(raw_data)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No missing values found!\")\n",
    "            \n",
    "    elif isinstance(raw_data, list):\n",
    "        print(\"üìä First 3 items:\")\n",
    "        for i, item in enumerate(raw_data[:3], 1):\n",
    "            print(f\"\\nItem {i}:\")\n",
    "            for key, value in item.items():\n",
    "                preview = str(value)[:100] + \"...\" if len(str(value)) > 100 else str(value)\n",
    "                print(f\"   {key}: {preview}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to preview. Please load your dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 3: Process and Clean the Data\n",
    "\n",
    "Now let's process the raw data into our standard Q&A format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "if loader is not None:\n",
    "    print(\"üîß PROCESSING DATA\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    try:\n",
    "        # Process data to Q&A format\n",
    "        processed_data = loader.process_data()\n",
    "        \n",
    "        print(f\"‚úÖ Data processed successfully!\")\n",
    "        print(f\"üìä Processed {len(processed_data)} Q&A pairs\")\n",
    "        \n",
    "        # Get statistics\n",
    "        stats = loader.get_data_statistics()\n",
    "        \n",
    "        print(\"\\nüìà DATASET STATISTICS:\")\n",
    "        print(\"-\"*30)\n",
    "        for key, value in stats.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"{key}: {len(value)} categories\")\n",
    "                # Show top categories\n",
    "                top_cats = list(value.items())[:5]\n",
    "                for cat, count in top_cats:\n",
    "                    print(f\"   - {cat}: {count}\")\n",
    "                if len(value) > 5:\n",
    "                    print(f\"   ... and {len(value)-5} more\")\n",
    "            elif isinstance(value, float):\n",
    "                print(f\"{key}: {value:.2f}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing data: {e}\")\n",
    "        processed_data = None\n",
    "        stats = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No loader available. Please load your dataset first.\")\n",
    "    processed_data = None\n",
    "    stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview processed data\n",
    "if processed_data:\n",
    "    print(\"üëÄ PROCESSED DATA PREVIEW\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    preview_samples = min(5, len(processed_data))\n",
    "    \n",
    "    for i, item in enumerate(processed_data[:preview_samples], 1):\n",
    "        print(f\"\\nüìù Sample {i}:\")\n",
    "        print(f\"   Q: {item['question']}\")\n",
    "        print(f\"   A: {item['answer'][:150]}{'...' if len(item['answer']) > 150 else ''}\")\n",
    "        if 'category' in item:\n",
    "            print(f\"   Category: {item['category']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    if len(processed_data) > preview_samples:\n",
    "        print(f\"\\n... and {len(processed_data) - preview_samples} more samples\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processed data to preview.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Data Visualization and Analysis\n",
    "\n",
    "Let's create some visualizations to better understand your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if processed_data and len(processed_data) > 0:\n",
    "    print(\"üìä CREATING VISUALIZATIONS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Extract data for analysis\n",
    "    questions = [item['question'] for item in processed_data]\n",
    "    answers = [item['answer'] for item in processed_data]\n",
    "    categories = [item.get('category', 'Unknown') for item in processed_data]\n",
    "    \n",
    "    # Calculate lengths\n",
    "    question_lengths = [len(q.split()) for q in questions]\n",
    "    answer_lengths = [len(a.split()) for a in answers]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Healthcare Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Question length distribution\n",
    "    axes[0, 0].hist(question_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Question Length Distribution')\n",
    "    axes[0, 0].set_xlabel('Number of Words')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(question_lengths), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(question_lengths):.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Answer length distribution\n",
    "    axes[0, 1].hist(answer_lengths, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_title('Answer Length Distribution')\n",
    "    axes[0, 1].set_xlabel('Number of Words')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].axvline(np.mean(answer_lengths), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(answer_lengths):.1f}')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Category distribution (if available)\n",
    "    category_counts = Counter(categories)\n",
    "    if len(category_counts) > 1 and 'Unknown' not in category_counts or len(category_counts) > 1:\n",
    "        # Show top 10 categories\n",
    "        top_categories = category_counts.most_common(10)\n",
    "        cats, counts = zip(*top_categories)\n",
    "        \n",
    "        axes[1, 0].bar(range(len(cats)), counts, color='lightcoral')\n",
    "        axes[1, 0].set_title('Top Medical Categories')\n",
    "        axes[1, 0].set_xlabel('Categories')\n",
    "        axes[1, 0].set_ylabel('Number of Q&A Pairs')\n",
    "        axes[1, 0].set_xticks(range(len(cats)))\n",
    "        axes[1, 0].set_xticklabels(cats, rotation=45, ha='right')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No category information\\navailable', \n",
    "                        ha='center', va='center', transform=axes[1, 0].transAxes,\n",
    "                        fontsize=12)\n",
    "        axes[1, 0].set_title('Medical Categories')\n",
    "    \n",
    "    # 4. Question vs Answer length scatter\n",
    "    axes[1, 1].scatter(question_lengths, answer_lengths, alpha=0.6, color='purple')\n",
    "    axes[1, 1].set_title('Question vs Answer Length')\n",
    "    axes[1, 1].set_xlabel('Question Length (words)')\n",
    "    axes[1, 1].set_ylabel('Answer Length (words)')\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    correlation = np.corrcoef(question_lengths, answer_lengths)[0, 1]\n",
    "    axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                    transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualizations created!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed statistics table\n",
    "if processed_data:\n",
    "    print(\"üìä DETAILED STATISTICS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Create statistics DataFrame\n",
    "    stats_data = {\n",
    "        'Metric': [\n",
    "            'Total Q&A Pairs',\n",
    "            'Avg Question Length',\n",
    "            'Avg Answer Length',\n",
    "            'Min Question Length',\n",
    "            'Max Question Length',\n",
    "            'Min Answer Length',\n",
    "            'Max Answer Length',\n",
    "            'Unique Categories'\n",
    "        ],\n",
    "        'Value': [\n",
    "            len(processed_data),\n",
    "            f\"{np.mean(question_lengths):.1f} words\",\n",
    "            f\"{np.mean(answer_lengths):.1f} words\",\n",
    "            f\"{min(question_lengths)} words\",\n",
    "            f\"{max(question_lengths)} words\",\n",
    "            f\"{min(answer_lengths)} words\",\n",
    "            f\"{max(answer_lengths)} words\",\n",
    "            len(set(categories)) if categories else 'N/A'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    display(stats_df)\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # Check for very short questions/answers\n",
    "    short_questions = sum(1 for q in question_lengths if q < 3)\n",
    "    short_answers = sum(1 for a in answer_lengths if a < 5)\n",
    "    \n",
    "    quality_checks = [\n",
    "        (\"Questions too short (<3 words)\", short_questions, short_questions == 0),\n",
    "        (\"Answers too short (<5 words)\", short_answers, short_answers == 0),\n",
    "        (\"Dataset size adequate (>50 pairs)\", len(processed_data), len(processed_data) > 50),\n",
    "        (\"Good variety (>5 categories)\", len(set(categories)), len(set(categories)) > 5),\n",
    "    ]\n",
    "    \n",
    "    for check, value, is_good in quality_checks:\n",
    "        icon = \"‚úÖ\" if is_good else \"‚ö†Ô∏è\"\n",
    "        print(f\"{icon} {check}: {value}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    good_checks = sum(1 for _, _, is_good in quality_checks if is_good)\n",
    "    total_checks = len(quality_checks)\n",
    "    \n",
    "    print(f\"\\nüìä Overall Quality Score: {good_checks}/{total_checks} ({good_checks/total_checks*100:.0f}%)\")\n",
    "    \n",
    "    if good_checks >= 3:\n",
    "        print(\"üéâ Your dataset looks good for training!\")\n",
    "    elif good_checks >= 2:\n",
    "        print(\"üëç Your dataset is usable but could be improved.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Your dataset may need some improvements before training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 5: Save Processed Dataset\n",
    "\n",
    "Let's save the processed dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "if processed_data and loader:\n",
    "    print(\"üíæ SAVING PROCESSED DATASET\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Define output path\n",
    "    output_path = '/workspace/data/kaggle_medical_dataset.json'\n",
    "    \n",
    "    try:\n",
    "        # Save processed data\n",
    "        loader.save_processed_data(output_path)\n",
    "        \n",
    "        print(f\"‚úÖ Processed dataset saved to: {output_path}\")\n",
    "        print(f\"üìä Saved {len(processed_data)} Q&A pairs\")\n",
    "        \n",
    "        # Verify the saved file\n",
    "        with open(output_path, 'r') as f:\n",
    "            saved_data = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Verification: {len(saved_data)} pairs loaded from saved file\")\n",
    "        \n",
    "        # Show file size\n",
    "        file_size = os.path.getsize(output_path) / 1024  # KB\n",
    "        print(f\"üìÅ File size: {file_size:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving dataset: {e}\")\n",
    "        output_path = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processed data to save.\")\n",
    "    output_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 6: Next Steps\n",
    "\n",
    "Great! You've successfully explored your dataset. Here's what to do next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ DATA EXPLORATION COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if processed_data and output_path:\n",
    "    print(f\"‚úÖ Successfully processed {len(processed_data)} medical Q&A pairs\")\n",
    "    print(f\"üíæ Data saved to: {output_path}\")\n",
    "    \n",
    "    print(\"\\nüöÄ NEXT STEPS:\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"1. üéì Train Your Model:\")\n",
    "    print(\"   ‚Üí Open: notebooks/03_Model_Training.ipynb\")\n",
    "    print(\"   ‚Üí Train a healthcare chatbot on your data\")\n",
    "    \n",
    "    print(\"\\n2. üèÉ‚Äç‚ôÇÔ∏è Quick Training (Alternative):\")\n",
    "    print(f\"   ‚Üí Run: python train_kaggle_chatbot.py --kaggle_dataset_path {DATASET_PATH}\")\n",
    "    \n",
    "    print(\"\\n3. üìä Training Recommendations:\")\n",
    "    if len(processed_data) < 100:\n",
    "        print(\"   ‚Üí Use: --model_key distilgpt2 (faster for small datasets)\")\n",
    "        print(\"   ‚Üí Use: --epochs 2 (fewer epochs for small data)\")\n",
    "    elif len(processed_data) < 1000:\n",
    "        print(\"   ‚Üí Use: --model_key dialogpt-medium (good balance)\")\n",
    "        print(\"   ‚Üí Use: --epochs 3 (standard training)\")\n",
    "    else:\n",
    "        print(\"   ‚Üí Use: --model_key dialogpt-medium (best quality)\")\n",
    "        print(\"   ‚Üí Use: --batch_size 2 (for large datasets)\")\n",
    "        print(\"   ‚Üí Consider: --max_samples 2000 (to limit training time)\")\n",
    "    \n",
    "    print(\"\\nüí° TIP: Start with a quick test training first!\")\n",
    "    print(f\"   python train_kaggle_chatbot.py \\\\\")\n",
    "    print(f\"     --kaggle_dataset_path {DATASET_PATH} \\\\\")\n",
    "    print(f\"     --epochs 1 --max_samples 100 --model_key distilgpt2\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Data exploration incomplete.\")\n",
    "    print(\"\\nüîß TROUBLESHOOTING:\")\n",
    "    print(\"-\"*20)\n",
    "    print(\"1. Make sure DATASET_PATH is set correctly\")\n",
    "    print(\"2. Check that your dataset file exists and is readable\")\n",
    "    print(\"3. Verify your dataset has question and answer columns\")\n",
    "    print(\"4. Try uploading a different dataset file\")\n",
    "\n",
    "print(\"\\nüìö RESOURCES:\")\n",
    "print(\"-\"*15)\n",
    "print(\"‚Ä¢ Full Guide: KAGGLE_DATASET_GUIDE.md\")\n",
    "print(\"‚Ä¢ Quick Start: YOUR_KAGGLE_DATASET_INSTRUCTIONS.md\")\n",
    "print(\"‚Ä¢ Next Notebook: 03_Model_Training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}